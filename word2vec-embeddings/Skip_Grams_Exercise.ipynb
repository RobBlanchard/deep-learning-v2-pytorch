{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram Word2Vec\n",
    "\n",
    "In this notebook, I'll lead you through using PyTorch to implement the [Word2Vec algorithm](https://en.wikipedia.org/wiki/Word2vec) using the skip-gram architecture. By implementing this, you'll learn about embedding words for use in natural language processing. This will come in handy when dealing with things like machine translation.\n",
    "\n",
    "## Readings\n",
    "\n",
    "Here are the resources I used to build this notebook. I suggest reading these either beforehand or while you're working on this material.\n",
    "\n",
    "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of Word2Vec from Chris McCormick \n",
    "* [First Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "* [Neural Information Processing Systems, paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for Word2Vec also from Mikolov et al.\n",
    "\n",
    "---\n",
    "## Word embeddings\n",
    "\n",
    "When you're dealing with words in text, you end up with tens of thousands of word classes to analyze; one for each word in a vocabulary. Trying to one-hot encode these words is massively inefficient because most values in a one-hot vector will be set to zero. So, the matrix multiplication that happens in between a one-hot input vector and a first, hidden layer will result in mostly zero-valued hidden outputs.\n",
    "\n",
    "<img src='assets/one_hot_encoding.png' width=50%>\n",
    "\n",
    "To solve this problem and greatly increase the efficiency of our networks, we use what are called **embeddings**. Embeddings are just a fully connected layer like you've seen before. We call this layer the embedding layer and the weights are embedding weights. We skip the multiplication into the embedding layer by instead directly grabbing the hidden layer values from the weight matrix. We can do this because the multiplication of a one-hot encoded vector with a matrix returns the row of the matrix corresponding the index of the \"on\" input unit.\n",
    "\n",
    "<img src='assets/lookup_matrix.png' width=50%>\n",
    "\n",
    "Instead of doing the matrix multiplication, we use the weight matrix as a lookup table. We encode the words as integers, for example \"heart\" is encoded as 958, \"mind\" as 18094. Then to get hidden layer values for \"heart\", you just take the 958th row of the embedding matrix. This process is called an **embedding lookup** and the number of hidden units is the **embedding dimension**.\n",
    "\n",
    "<img src='assets/tokenize_lookup.png' width=50%>\n",
    " \n",
    "There is nothing magical going on here. The embedding lookup table is just a weight matrix. The embedding layer is just a hidden layer. The lookup is just a shortcut for the matrix multiplication. The lookup table is trained just like any weight matrix.\n",
    "\n",
    "Embeddings aren't only used for words of course. You can use them for any model where you have a massive number of classes. A particular type of model called **Word2Vec** uses the embedding layer to find vector representations of words that contain semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Word2Vec\n",
    "\n",
    "The Word2Vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words.\n",
    "\n",
    "<img src=\"assets/context_drink.png\" width=40%>\n",
    "\n",
    "Words that show up in similar **contexts**, such as \"coffee\", \"tea\", and \"water\" will have vectors near each other. Different words will be further away from one another, and relationships can be represented by distance in vector space.\n",
    "\n",
    "<img src=\"assets/vector_distance.png\" width=40%>\n",
    "\n",
    "\n",
    "There are two architectures for implementing Word2Vec:\n",
    ">* CBOW (Continuous Bag-Of-Words) and \n",
    "* Skip-gram\n",
    "\n",
    "<img src=\"assets/word2vec_architectures.png\" width=60%>\n",
    "\n",
    "In this implementation, we'll be using the **skip-gram architecture** because it performs better than CBOW. Here, we pass in a word and try to predict the words surrounding it in the text. In this way, we can train the network to learn representations for words that show up in similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Data\n",
    "\n",
    "Next, we'll ask you to load in data and place it in the `data` directory\n",
    "\n",
    "1. Load the [text8 dataset](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip); a file of cleaned up *Wikipedia article text* from Matt Mahoney. \n",
    "2. Place that data in the `data` folder in the home directory.\n",
    "3. Then you can extract it and delete the archive, zip file to save storage space.\n",
    "\n",
    "After following these steps, you should have one file in your data directory: `data/text8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "# read in the extracted text file      \n",
    "with open('data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Here I'm fixing up the text to make training easier. This comes from the `utils.py` file. The `preprocess` function does a few things:\n",
    ">* It converts any punctuation into tokens, so a period is changed to ` <PERIOD> `. In this data set, there aren't any periods, but it will help in other NLP problems. \n",
    "* It removes all words that show up five or *fewer* times in the dataset. This will greatly reduce issues due to noise in the data and improve the quality of the vector representations. \n",
    "* It returns a list of words in the text.\n",
    "\n",
    "This may take a few seconds to run, since our text file is quite large. If you want to write your own functions for this stuff, go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# get list of words\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in text: 16680599\n",
      "Unique words: 63641\n"
     ]
    }
   ],
   "source": [
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Next, I'm creating two dictionaries to convert words to integers and back again (integers to words). This is again done with a function in the `utils.py` file. `create_lookup_tables` takes in a list of words in a text and returns two dictionaries.\n",
    ">* The integers are assigned in descending frequency order, so the most frequent word (\"the\") is given the integer 0 and the next most frequent is 1, and so on. \n",
    "\n",
    "Once we have our dictionaries, the words are converted to integers and stored in the list `int_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580]\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by \n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{1*10^6/16*10^6}} = 0.98735 $$\n",
    "\n",
    "I'm going to leave this up to you as an exercise. Check out my solution to see how I did it.\n",
    "\n",
    "> **Exercise:** Implement subsampling for the words in `int_words`. That is, go through `int_words` and discard each word given the probablility $P(w_i)$ shown above. Note that $P(w_i)$ is the probability that a word is discarded. Assign the subsampled data to `train_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12052786\n",
      "[5233, 11, 5, 194, 1, 45, 58, 127, 741, 476, 133, 0, 1, 0, 102]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def subsample(int_words, threshold=1e-5):\n",
    "    word_counts = Counter(int_words)\n",
    "    total_words = len(int_words)\n",
    "\n",
    "    # discard some frequent words, according to the subsampling equation\n",
    "    # create a new list of words for training\n",
    "    p_words = {w : np.sqrt(threshold/(word_counts[w]/total_words)) for w in word_counts}\n",
    "    return [w for w in int_words if random.random() > p_words[w]]\n",
    "    \n",
    "train_words = subsample(int_words)\n",
    "print(len(train_words))\n",
    "print(train_words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is in good shape, we need to get it into the proper form to pass it into our network. With the skip-gram architecture, for each word in the text, we want to define a surrounding _context_ and grab all the words in a window around that word, with size $C$. \n",
    "\n",
    "From [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf): \n",
    "\n",
    "\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose $C = 5$, for each training word we will select randomly a number $R$ in range $[ 1: C ]$, and then use $R$ words from history and $R$ words from the future of the current word as correct labels.\"\n",
    "\n",
    "> **Exercise:** Implement a function `get_target` that receives a list of words, an index, and a window size, then returns a list of words in the window around the index. Make sure to use the algorithm described above, where you chose a random number of words to from the window.\n",
    "\n",
    "Say, we have an input and we're interested in the idx=2 token, `741`: \n",
    "```\n",
    "[5233, 58, 741, 10571, 27349, 0, 15067, 58112, 3580, 58, 10712]\n",
    "```\n",
    "\n",
    "For `R=2`, `get_target` should return a list of four values:\n",
    "```\n",
    "[5233, 58, 10571, 27349]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    R = random.choice(range(1,window_size+1))\n",
    "\n",
    "    previous_words = words[max(0,idx-R):idx]\n",
    "    next_words = words[idx+1:idx+R+1]\n",
    "    \n",
    "    return previous_words + next_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Target:  [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# test your code!\n",
    "\n",
    "# run this cell multiple times to check for random window selection\n",
    "int_text = [i for i in range(10)]\n",
    "print('Input: ', int_text)\n",
    "idx=9 # word index of interest\n",
    "\n",
    "target = get_target(int_text, idx=idx, window_size=12)\n",
    "print('Target: ', target)  # you should get some indices around the idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Batches \n",
    "\n",
    "Here's a generator function that returns batches of input and target data for our model, using the `get_target` function from above. The idea is that it grabs `batch_size` words from a words list. Then for each of those batches, it gets the target words in a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_target([0,1,2,3],3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\n",
      "y\n",
      " [1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "int_text = [i for i in range(20)]\n",
    "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph\n",
    "\n",
    "Below is an approximate diagram of the general structure of our network.\n",
    "<img src=\"assets/skip_gram_arch.png\" width=60%>\n",
    "\n",
    ">* The input words are passed in as batches of input word tokens. \n",
    "* This will go into a hidden layer of linear units (our embedding layer). \n",
    "* Then, finally into a softmax output layer. \n",
    "\n",
    "We'll use the softmax layer to make a prediction about the context words by sampling, as usual.\n",
    "\n",
    "The idea here is to train the embedding layer weight matrix to find efficient representations for our words. We can discard the softmax layer because we don't really care about making predictions with this network. We just want the embedding matrix so we can use it in _other_ networks we build using this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Here, I'm creating a function that will help us observe our model as it learns. We're going to choose a few common words and few uncommon words. Then, we'll print out the closest words to them using the cosine similarity: \n",
    "\n",
    "<img src=\"assets/two_vectors.png\" width=30%>\n",
    "\n",
    "$$\n",
    "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
    "$$\n",
    "\n",
    "\n",
    "We can encode the validation words as vectors $\\vec{a}$ using the embedding table, then calculate the similarity with each word vector $\\vec{b}$ in the embedding table. With the similarities, we can print out the validation words and words in our embedding table semantically similar to those words. It's a nice way to check that our embedding table is grouping together words with similar semantic meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram model\n",
    "\n",
    "Define and train the SkipGram model. \n",
    "> You'll need to define an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) and a final, softmax output layer.\n",
    "\n",
    "An Embedding layer takes in a number of inputs, importantly:\n",
    "* **num_embeddings** – the size of the dictionary of embeddings, or how many rows you'll want in the embedding weight matrix\n",
    "* **embedding_dim** – the size of each embedding vector; the embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_embed, n_vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 2, 2] [1, 2, 0, 2, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7377, -4.3389, -4.0445, -3.1373, -2.0747, -2.4299, -2.3668, -3.0537,\n",
       "         -3.4564, -3.4212, -3.0319, -4.2764, -3.3396, -3.1971, -3.8073, -2.6532,\n",
       "         -3.5357, -2.3384, -3.0587, -3.2483],\n",
       "        [-2.7377, -4.3389, -4.0445, -3.1373, -2.0747, -2.4299, -2.3668, -3.0537,\n",
       "         -3.4564, -3.4212, -3.0319, -4.2764, -3.3396, -3.1971, -3.8073, -2.6532,\n",
       "         -3.5357, -2.3384, -3.0587, -3.2483],\n",
       "        [-3.0815, -4.2840, -3.7032, -3.0261, -1.7181, -2.8544, -2.1721, -3.3445,\n",
       "         -3.7442, -3.6303, -3.0396, -3.4394, -3.8644, -3.2453, -3.4319, -2.8017,\n",
       "         -3.4335, -2.7871, -2.6674, -3.3640],\n",
       "        [-3.0815, -4.2840, -3.7032, -3.0261, -1.7181, -2.8544, -2.1721, -3.3445,\n",
       "         -3.7442, -3.6303, -3.0396, -3.4394, -3.8644, -3.2453, -3.4319, -2.8017,\n",
       "         -3.4335, -2.7871, -2.6674, -3.3640],\n",
       "        [-2.2309, -3.3046, -3.0485, -3.8709, -2.6241, -2.6943, -3.0401, -3.1955,\n",
       "         -3.1579, -3.0092, -3.1295, -3.3748, -3.0308, -3.4729, -2.9672, -2.4943,\n",
       "         -3.4320, -2.5158, -3.1750, -3.8787],\n",
       "        [-2.2309, -3.3046, -3.0485, -3.8709, -2.6241, -2.6943, -3.0401, -3.1955,\n",
       "         -3.1579, -3.0092, -3.1295, -3.3748, -3.0308, -3.4729, -2.9672, -2.4943,\n",
       "         -3.4320, -2.5158, -3.1750, -3.8787]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg = SkipGram(len(set(int_text)),3)\n",
    "x,y = next(get_batches(int_text, 3))\n",
    "print(x,y)\n",
    "\n",
    "sg(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4506, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nn.NLLLoss()\n",
    "target = torch.Tensor(y)\n",
    "target = target.type(torch.LongTensor)\n",
    "test(sg(torch.tensor(x)), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Below is our training loop, and I recommend that you train on GPU, if available.\n",
    "\n",
    "**Note that, because we applied a softmax function to our model output, we are using NLLLoss** as opposed to cross entropy. This is because Softmax  in combination with NLLLoss = CrossEntropy loss ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "their  |  respectable multipurpose acute workmanship lasers\n",
      "seven  |  labelled gerrit pullback overturn oromo\n",
      "if  |  prophetic showers cartilage gora bg\n",
      "which  |  bookstores edmondson djbdns appended hells\n",
      "time  |  iban istituto cooling madam interferes\n",
      "will  |  invoices identifications lachmann kochi falcons\n",
      "and  |  travers tiptree attachments satirizes gyrus\n",
      "in  |  enshrine nagasaki pledged royalty rrna\n",
      "file  |  adad concubine chagos boito sheldrake\n",
      "quite  |  goldsmiths balke abusing junnin lemma\n",
      "nobel  |  subsided rossa goalscorer miyagi mobilization\n",
      "bible  |  degenerate sentencing rushmore salutis paw\n",
      "san  |  pendleton centralisation damian dulcinea reappear\n",
      "something  |  salvage foundationalism bootp tortola albano\n",
      "smith  |  fetches plumb soundly hydrogenation manipulation\n",
      "pressure  |  formant coils pound malfunctioning lick\n",
      "...\n",
      "so  |  optimised mushes reus petitioning avot\n",
      "not  |  pngimage metropolis nonconformists hominis kohlberg\n",
      "eight  |  ivanhoe masterpieces fads explosive canonised\n",
      "about  |  corrino centralization moderately automated granitic\n",
      "more  |  nihilism parcham sld good lui\n",
      "into  |  hob contentious narrowness animist katzenjammer\n",
      "with  |  appalachian dsph plush zealanders juridical\n",
      "used  |  lacking kaiserreich sibylla druyan tenn\n",
      "operating  |  delete equating dodgers moser magnus\n",
      "brother  |  rebuked plus rey repopulated vocalized\n",
      "rise  |  practices similarity staining spit vijayanagara\n",
      "professional  |  nz retains expandable engrailed muhi\n",
      "bible  |  degenerate sentencing rushmore salutis paw\n",
      "mathematics  |  tigress parodying curators kinetic mckusick\n",
      "applied  |  morpheme haitians gnumeric numismatic pervez\n",
      "operations  |  outings nestled tuileries gneiss ntp\n",
      "...\n",
      "new  |  tibetans leyden amphibious euglenids bgcolor\n",
      "on  |  bayes meets petrels cl megalithic\n",
      "states  |  langland bracks rumford unexplained harmonicas\n",
      "the  |  conspiracies paulsen rdx corvus warners\n",
      "nine  |  someplace hornsby kamen germanium judicially\n",
      "over  |  christoph cad delineate untrue icaza\n",
      "american  |  fenech mescaline tsvetaeva alfheim restructure\n",
      "so  |  optimised mushes reus petitioning avot\n",
      "discovered  |  playlist juggalos zhivago manhunter taino\n",
      "shown  |  abu monotheist monogamy shaved envelop\n",
      "troops  |  stat denounce owners negatives workarounds\n",
      "assembly  |  cocktail hoard degli nymphs bayreuth\n",
      "ice  |  cotabato winsor marketshare communicating oskar\n",
      "ocean  |  karlovac pasts endowment lack printings\n",
      "additional  |  endomorphisms belgrade rtf cong analecta\n",
      "pre  |  almost uke catacombs comparably hegelian\n",
      "...\n",
      "see  |  linn grove urbina guide fluorspar\n",
      "their  |  respectable multipurpose acute lasers perverse\n",
      "about  |  corrino centralization moderately automated granitic\n",
      "many  |  polycarp ley spiteful got procellariiformes\n",
      "five  |  mcginley saddles ghosting sweeping nessie\n",
      "states  |  langland rumford unexplained bracks harmonicas\n",
      "there  |  raffarin palatalization baldness macfarquhar yerba\n",
      "th  |  tackled platinum gasket express jimmy\n",
      "square  |  piecewise shackleton sudeten dragons phenotype\n",
      "san  |  pendleton centralisation josephus reappear damian\n",
      "articles  |  zulus deformed filming hunters demetrius\n",
      "defense  |  ericaceae flagstaff carve posek interned\n",
      "shown  |  abu monotheist monogamy shaved envelop\n",
      "http  |  sarkozy scythians augmented guybrush murakami\n",
      "except  |  maasai jacobus mikhalkov achiral thrash\n",
      "active  |  drapery startposition horner earthsea michell\n",
      "...\n",
      "war  |  inspiring crumbs crutches fantasyland megabytes\n",
      "seven  |  labelled gerrit pullback overturn oromo\n",
      "the  |  conspiracies rdx corvus confiscating knut\n",
      "these  |  hair hashem oscillating epimetheus chronicled\n",
      "no  |  aime stumbles chokes stupor utilization\n",
      "first  |  recuperate resolved automaker quatre jaggies\n",
      "as  |  euskadi rodgers mice ashkenazic maia\n",
      "when  |  elman genre horrendous altering cai\n",
      "institute  |  lombardic taxpayer aquilonia qdos rushed\n",
      "derived  |  meticulously dort riaa breakbeat weird\n",
      "operating  |  equating delete dodgers haller magnus\n",
      "stage  |  interpol ignazio tte mrna dashes\n",
      "know  |  memeplexes aspirin oxley forsythe millenarianism\n",
      "proposed  |  decade jour grimoire explains pragmatically\n",
      "assembly  |  cocktail hoard nymphs slovenia garz\n",
      "joseph  |  senka machina pointy udinese gdr\n",
      "...\n",
      "are  |  professor chariot brl stevenson kassa\n",
      "over  |  christoph untrue delineate accentuated icaza\n",
      "if  |  prophetic showers cartilage gora lerner\n",
      "not  |  pngimage metropolis nonconformists hominis kohlberg\n",
      "they  |  glycosidic quivering happily cocalus adjusts\n",
      "seven  |  labelled gerrit pullback overturn oromo\n",
      "with  |  appalachian dsph plush dddddd juridical\n",
      "i  |  jakobson grands buckwheat pikes maiden\n",
      "numerous  |  frown dassault better boris karin\n",
      "nobel  |  subsided mobilization rossa miyagi gch\n",
      "award  |  commando snack recommendation barbra hines\n",
      "recorded  |  langston catapult subroutine leaky resolver\n",
      "universe  |  dks finlandization titration trackless turkeys\n",
      "operating  |  equating delete dodgers haller magnus\n",
      "report  |  jenkins nilgiri carrots mmol outcry\n",
      "police  |  intimate leninism started seagram exercising\n",
      "...\n",
      "so  |  optimised mushes reus southwark petitioning\n",
      "b  |  immortalized newly heteronormativity chroniclers england\n",
      "but  |  duelling sisters paralipomena ziyad beadle\n",
      "has  |  priscillian ansi madras torque winged\n",
      "often  |  apex boreal escalated incompatibilities whatsoever\n",
      "eight  |  ivanhoe masterpieces fads nutation explosive\n",
      "i  |  jakobson grands buckwheat pikes bene\n",
      "not  |  pngimage nonconformists metropolis hominis kohlberg\n",
      "hold  |  ryaku pregnant lawhead ign numbering\n",
      "powers  |  havana birger chains ana hond\n",
      "older  |  carburetor incessantly demonic lucidity armenia\n",
      "pope  |  coelacanth gec ques mines inferno\n",
      "alternative  |  conspiring idyllic ghq creasy transformer\n",
      "egypt  |  organises candy wylie chinese altai\n",
      "universe  |  dks finlandization titration trackless turkeys\n",
      "freedom  |  expounding juries bright downers whirl\n",
      "...\n",
      "up  |  krishnan battling hofmann support thea\n",
      "used  |  lacking kaiserreich druyan sibylla cimbri\n",
      "th  |  tackled gasket express platinum jimmy\n",
      "however  |  audiogalaxy macron gawk spitfire balke\n",
      "five  |  mcginley saddles sweeping ghosting nessie\n",
      "in  |  royalty enshrine pledged nagasaki attendant\n",
      "between  |  warmblood angus emergency attributed liberals\n",
      "united  |  westley lehman ladislas hpc suborder\n",
      "file  |  adad chagos concubine sheldrake boito\n",
      "police  |  intimate leninism seagram started exercising\n",
      "bill  |  negroids dominus walks rantissi kayaks\n",
      "centre  |  recipients overtaking liking dildo ladykillers\n",
      "gold  |  prizren ringed volunteering vaughn dvg\n",
      "engine  |  panza pacific restful palatine snowmobiles\n",
      "pressure  |  formant coils pound lick malfunctioning\n",
      "behind  |  xix marquette novel dyslexic pickering\n",
      "...\n",
      "there  |  raffarin palatalization baldness macfarquhar toaster\n",
      "only  |  reymond gregorio pillaging instill aware\n",
      "called  |  larp enthroned unconnected lenition cellar\n",
      "by  |  surfer stinger prevents metathesis suazo\n",
      "would  |  sov sinh communally mamie gloster\n",
      "four  |  moderator parasitic oilseed digweed calabria\n",
      "were  |  gmc goldman travolta sensation goldstein\n",
      "but  |  duelling sisters paralipomena ziyad beadle\n",
      "road  |  conspiracies inherits peeling flanker stressful\n",
      "channel  |  terrorists dares idealist deptford tinymux\n",
      "gold  |  prizren ringed volunteering vaughn dvg\n",
      "hold  |  ryaku pregnant lawhead ign numbering\n",
      "joseph  |  senka machina pointy udinese gdr\n",
      "prince  |  occidentalis macneille langland ewart foxy\n",
      "dr  |  husband gardner freenet japonica kaddish\n",
      "engineering  |  psychoanalytic disposed indexed pearlman plancherel\n",
      "...\n",
      "eight  |  ivanhoe masterpieces fads nutation canonised\n",
      "history  |  physique expropriated elliot alcindor banished\n",
      "more  |  nihilism lui sld good parcham\n",
      "first  |  recuperate resolved automaker quatre jaggies\n",
      "some  |  cheapest pio telomerase dulcian olivine\n",
      "seven  |  labelled gerrit pullback overturn oromo\n",
      "so  |  optimised mushes reus southwark petitioning\n",
      "by  |  surfer stinger prevents metathesis suazo\n",
      "hold  |  ryaku pregnant lawhead ign numbering\n",
      "pope  |  coelacanth gec ques mines greta\n",
      "creation  |  welcomes corsa encompasses decrease tory\n",
      "smith  |  fetches plumb hydrogenation bologna soundly\n",
      "hit  |  balkan rahxephon confessing ashok operatorname\n",
      "ocean  |  karlovac endowment pasts lack printings\n",
      "additional  |  endomorphisms belgrade cong analecta rtf\n",
      "running  |  misplaced dragging postulate devoiced copps\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "their  |  respectable multipurpose acute perverse lasers\n",
      "such  |  samkhya bottlers curonian flipped everlast\n",
      "some  |  cheapest pio telomerase dulcian olivine\n",
      "states  |  langland rumford bracks unexplained harmonicas\n",
      "history  |  physique expropriated elliot alcindor banished\n",
      "or  |  longa mingw parsley descendents goddamn\n",
      "of  |  spark quelques avengers desperate ccd\n",
      "other  |  schama martyrs mutate eph clinicians\n",
      "dr  |  husband gardner freenet japonica kaddish\n",
      "gold  |  prizren ringed volunteering vaughn dvg\n",
      "know  |  memeplexes aspirin forsythe oxley nickleby\n",
      "heavy  |  possibility repubblica pivot dale cornerstones\n",
      "http  |  sarkozy murakami neoplatonic hunmin titian\n",
      "joseph  |  senka machina pointy udinese gdr\n",
      "bbc  |  constitutes complexion repudiating naturae allo\n",
      "smith  |  fetches plumb hydrogenation bologna soundly\n",
      "...\n",
      "can  |  soissons weiss linkage apprenticeship travel\n",
      "i  |  jakobson grands buckwheat pikes bene\n",
      "b  |  immortalized newly heteronormativity chroniclers confounded\n",
      "had  |  har partitioning kenji thebaid neutronic\n",
      "two  |  wrested diverse hailie gpl adorning\n",
      "united  |  westley ladislas lehman suborder hpc\n",
      "is  |  unrealized trope morphy domitian brodsky\n",
      "these  |  hair oscillating hashem chronicled epimetheus\n",
      "versions  |  cb aman owed tampa soga\n",
      "woman  |  wore skilful kills autumn surmounted\n",
      "heavy  |  possibility repubblica pivot dale cornerstones\n",
      "experience  |  carolinas riso igreja undesired raid\n",
      "question  |  saurischia jarrett lauded kerosene bonner\n",
      "assembly  |  cocktail hoard nymphs innards garz\n",
      "engine  |  panza pacific restful palatine snowmobiles\n",
      "gold  |  prizren ringed volunteering vaughn dvg\n",
      "...\n",
      "but  |  duelling sisters ziyad paralipomena jobs\n",
      "been  |  charlize cloudy repudiating counselor electoral\n",
      "some  |  cheapest pio dulcian telomerase olivine\n",
      "they  |  glycosidic cocalus quivering happily adjusts\n",
      "only  |  reymond gregorio pillaging instill aware\n",
      "no  |  aime stumbles utilization chokes longford\n",
      "i  |  jakobson grands buckwheat pikes bene\n",
      "such  |  samkhya bottlers curonian flipped everlast\n",
      "event  |  pricing enumeration latitudes trans radon\n",
      "report  |  outcry carrots nilgiri jenkins length\n",
      "engine  |  panza pacific restful palatine snowmobiles\n",
      "know  |  memeplexes aspirin forsythe oxley nickleby\n",
      "pressure  |  formant coils pound lick malfunctioning\n",
      "ocean  |  karlovac endowment pasts lack printings\n",
      "question  |  saurischia jarrett lauded kerosene bonner\n",
      "consists  |  unpredictable correspondance sympathetic nh atsc\n",
      "...\n",
      "this  |  outtakes astrolabe allan complete manich\n",
      "when  |  elman genre horrendous altering cai\n",
      "time  |  istituto iban interferes cooling celeb\n",
      "th  |  tackled gasket express platinum chalk\n",
      "during  |  localization persians pieces economics langley\n",
      "had  |  har partitioning kenji thebaid neutronic\n",
      "used  |  kaiserreich lacking sibylla druyan cimbri\n",
      "not  |  pngimage nonconformists hominis kohlberg metropolis\n",
      "discovered  |  zhivago juggalos playlist gland lusitanian\n",
      "pressure  |  formant coils pound lick symbolising\n",
      "issue  |  waterford molyneux mouvement merv stomach\n",
      "universe  |  dks finlandization titration turkeys trackless\n",
      "award  |  commando snack recommendation barbra hines\n",
      "engine  |  panza pacific restful palatine snowmobiles\n",
      "heavy  |  possibility repubblica pivot dale cornerstones\n",
      "articles  |  zulus deformed filming hunters vertov\n",
      "...\n",
      "time  |  istituto iban interferes cooling celeb\n",
      "three  |  kona nonempty warhammer warhol financiers\n",
      "people  |  pharmacology suomenlinna lemmy mozambican bendix\n",
      "more  |  nihilism lui sld good parcham\n",
      "seven  |  labelled pullback gerrit overturn oromo\n",
      "in  |  royalty enshrine pledged nagasaki attendant\n",
      "be  |  auburn unorthodox afram skyrocketed embodying\n",
      "between  |  warmblood angus emergency liberals attributed\n",
      "powers  |  havana chains hond birger banco\n",
      "stage  |  interpol ignazio tte turbocharged equilateral\n",
      "question  |  saurischia jarrett lauded kerosene bonner\n",
      "police  |  intimate leninism exercising seagram kuna\n",
      "cost  |  hatfield scapegoat contraceptive jails malady\n",
      "derived  |  meticulously dort riaa throughput feburary\n",
      "recorded  |  langston catapult leaky subroutine resolver\n",
      "behind  |  xix novel marquette dyslexic pickering\n",
      "...\n",
      "known  |  accrue sweetcorn rested fanned sadness\n",
      "their  |  respectable acute multipurpose perverse chandragupta\n",
      "at  |  vegetative eagerness thessaly oup armenia\n",
      "and  |  tiptree attachments satirizes heretics travers\n",
      "over  |  christoph accentuated untrue acclamation icaza\n",
      "five  |  mcginley sweeping saddles ecuador nessie\n",
      "american  |  fenech tsvetaeva alfheim etudes mescaline\n",
      "so  |  optimised mushes reus southwark anta\n",
      "road  |  conspiracies inherits peeling flanker stressful\n",
      "pre  |  almost uke catacombs rapp murad\n",
      "know  |  memeplexes oxley aspirin forsythe herrmann\n",
      "operations  |  outings tuileries nestled tabasco snags\n",
      "egypt  |  organises wylie chinese limp altai\n",
      "smith  |  fetches hydrogenation plumb bologna soundly\n",
      "marriage  |  against paracelsus samarium ultimate withstood\n",
      "defense  |  ericaceae flagstaff francs carve posek\n",
      "...\n",
      "when  |  elman horrendous altering genre cai\n",
      "it  |  quinquatria colliery nodal expound ampersand\n",
      "to  |  antitank spine clanking intelligibility varieties\n",
      "which  |  bookstores edmondson djbdns appended tamed\n",
      "been  |  charlize repudiating cloudy counselor electoral\n",
      "an  |  intended liv friedland rosid masterworks\n",
      "in  |  royalty enshrine pledged nagasaki attendant\n",
      "into  |  hob katzenjammer instruments florid disloyalty\n",
      "prince  |  occidentalis macneille ewart langland netted\n",
      "nobel  |  subsided mobilization rossa goalscorer miyagi\n",
      "operations  |  outings tuileries nestled tabasco snags\n",
      "ice  |  cotabato winsor navigator marketshare communicating\n",
      "mainly  |  abidjan combats dnestr kedah apaches\n",
      "award  |  recommendation snack commando barbra musicological\n",
      "magazine  |  pedals quasimodo drowsiness pozna stanley\n",
      "engineering  |  psychoanalytic disposed plancherel indexed mothra\n",
      "...\n",
      "were  |  gmc goldman travolta goldstein sensation\n",
      "he  |  fry screwed remick disproved tons\n",
      "years  |  jealousy genuine anheuser comit elements\n",
      "or  |  longa mingw parsley descendents goddamn\n",
      "first  |  recuperate resolved quatre desist high\n",
      "are  |  professor chariot ave stevenson kassa\n",
      "american  |  fenech tsvetaeva alfheim etudes wacky\n",
      "which  |  bookstores edmondson djbdns appended tamed\n",
      "joseph  |  senka pointy machina gdr udinese\n",
      "consists  |  unpredictable correspondance sympathetic nh directorate\n",
      "applied  |  morpheme pervez haitians gnumeric numismatic\n",
      "numerous  |  frown dassault karin famine topping\n",
      "universe  |  dks finlandization titration trackless turkeys\n",
      "derived  |  meticulously dort riaa throughput feburary\n",
      "placed  |  rollercoaster executing israel marlborough pachacuti\n",
      "institute  |  lombardic taxpayer aquilonia arabic rushed\n",
      "...\n",
      "world  |  marquise responsive rhythm therein tungusic\n",
      "their  |  respectable acute chandragupta perverse multipurpose\n",
      "state  |  chernenko hayastan fsm aquinas vulpes\n",
      "for  |  shamir fire toscana cortege sodomy\n",
      "about  |  corrino automated centralization solicits granitic\n",
      "three  |  kona nonempty warhol warhammer mers\n",
      "s  |  piece grimmett chocolate tricking kaw\n",
      "called  |  larp enthroned lenition unconnected cellar\n",
      "alternative  |  conspiring idyllic ghq creasy tracks\n",
      "numerous  |  frown dassault karin famine topping\n",
      "discovered  |  juggalos playlist gland zhivago oscillate\n",
      "pressure  |  formant lick coils pound erlanger\n",
      "nobel  |  subsided rossa mobilization miyagi goalscorer\n",
      "smith  |  fetches hydrogenation plumb soundly bologna\n",
      "additional  |  endomorphisms belgrade gama cong analecta\n",
      "report  |  outcry nilgiri length shuts carrots\n",
      "...\n",
      "be  |  auburn unorthodox afram skyrocketed embodying\n",
      "some  |  pio dulcian telomerase cheapest olivine\n",
      "as  |  euskadi rodgers ashkenazic mice maia\n",
      "used  |  kaiserreich lacking sibylla cimbri druyan\n",
      "four  |  moderator oilseed digweed parasitic maize\n",
      "which  |  bookstores edmondson appended djbdns hells\n",
      "american  |  fenech tsvetaeva wacky alfheim romania\n",
      "new  |  tibetans leyden bgcolor euglenids amphibious\n",
      "applications  |  snowdon kyd matthieu pba wingtip\n",
      "shown  |  abu monotheist monogamy releasing shaved\n",
      "http  |  sarkozy ord neoplatonic titian murakami\n",
      "arts  |  ugly marks antacids bender exacerbated\n",
      "primarily  |  odors jordi punched logarithmically soy\n",
      "operations  |  outings nestled tuileries snags tsuburaya\n",
      "joseph  |  senka pointy udinese gdr machina\n",
      "report  |  nilgiri length outcry carrots shuts\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is  |  unrealized anathema trope rodham domitian\n",
      "will  |  invoices kochi identifications lachmann aalborg\n",
      "use  |  perennial quia faint raided graffiti\n",
      "one  |  carnivore ministerial neuch corporatism azrael\n",
      "into  |  hob katzenjammer florid everest instruments\n",
      "on  |  bayes meets petrels endosymbiont cl\n",
      "th  |  tackled express platinum gasket chalk\n",
      "called  |  larp enthroned unconnected lenition cellar\n",
      "know  |  memeplexes forsythe oxley aspirin herrmann\n",
      "resources  |  weekends amigos diaz historian cloaca\n",
      "taking  |  nolan natchez disparagingly dehydrogenase documented\n",
      "mainly  |  abidjan combats dnestr apaches kedah\n",
      "egypt  |  organises wylie chinese limp altai\n",
      "http  |  sarkozy ord neoplatonic titian murakami\n",
      "running  |  misplaced dragging copps ethnologue senado\n",
      "additional  |  endomorphisms belgrade gama cong analecta\n",
      "...\n",
      "has  |  priscillian ansi torque madras winged\n",
      "are  |  professor chariot ave stevenson kassa\n",
      "an  |  intended friedland liv rosid masterworks\n",
      "years  |  genuine comit jealousy anheuser elements\n",
      "for  |  fire shamir cortege sodomy toscana\n",
      "war  |  inspiring crumbs crutches lamborghini competed\n",
      "these  |  hair oscillating hashem chronicled epimetheus\n",
      "which  |  bookstores edmondson appended djbdns hells\n",
      "defense  |  ericaceae flagstaff francs posek carve\n",
      "ocean  |  karlovac pasts lack endowment wilder\n",
      "writers  |  reorganised lemonade prolifically hope insensitive\n",
      "shows  |  voyagers niddah feral tights pints\n",
      "existence  |  shangri shortcut eithne militaristic spokesmen\n",
      "assembly  |  cocktail innards hoard nymphs garz\n",
      "arts  |  ugly marks antacids bender exacerbated\n",
      "governor  |  code hazing siam seem enfant\n",
      "...\n",
      "five  |  mcginley saddles sweeping nessie ecuador\n",
      "people  |  pharmacology suomenlinna lemmy mozambican intentions\n",
      "up  |  krishnan envelop support battling thea\n",
      "it  |  quinquatria colliery nodal ampersand mixtec\n",
      "was  |  indictments kermit gama imaginary satyajit\n",
      "state  |  chernenko hayastan fsm aquinas vulpes\n",
      "there  |  raffarin baldness palatalization macfarquhar mahjong\n",
      "while  |  ounces neogrammarians aside stakeholder tech\n",
      "additional  |  endomorphisms belgrade gama cong analecta\n",
      "joseph  |  senka pointy udinese gdr machina\n",
      "pope  |  coelacanth gec mines greta science\n",
      "operating  |  delete equating haller dodgers conservationist\n",
      "paris  |  rundgren ureter neurons blower olmsted\n",
      "account  |  frits dimitrov lacaille sovnarkom zawahiri\n",
      "professional  |  nz engrailed caco muhi fiasco\n",
      "test  |  rink fontvieille runways oakland exchequer\n",
      "...\n",
      "his  |  arnage spaceballs sqrta icosahedron adair\n",
      "th  |  tackled express platinum gasket chalk\n",
      "two  |  wrested confusion dail hailie congruence\n",
      "also  |  polarizer bhopal bcc galante beirut\n",
      "s  |  piece grimmett tricking chocolate kaw\n",
      "states  |  harmonicas langland bracks kegan backward\n",
      "their  |  respectable chandragupta acute probation perverse\n",
      "an  |  intended friedland rosid liv masterworks\n",
      "stage  |  interpol ignazio tte turbocharged viscous\n",
      "http  |  sarkozy ord neoplatonic murakami hunmin\n",
      "engine  |  panza pacific restful palatine snowmobiles\n",
      "instance  |  aktion zedillo populated animate deva\n",
      "mean  |  succeeding sancti bhutan tenacious yungay\n",
      "additional  |  endomorphisms belgrade gama cong analecta\n",
      "recorded  |  langston catapult leaky triumphed subroutine\n",
      "hold  |  pregnant ryaku sainsbury numbering believer\n",
      "...\n",
      "many  |  polycarp spiteful korner italian lom\n",
      "as  |  rodgers euskadi mice ashkenazic maia\n",
      "with  |  appalachian plush dsph relegate dddddd\n",
      "was  |  indictments gama kermit griffey imaginary\n",
      "not  |  pngimage nonconformists rucker hominis brivla\n",
      "s  |  piece grimmett tricking chocolate deduces\n",
      "on  |  bayes meets milford endosymbiont cl\n",
      "see  |  guide centred apl merits despoiled\n",
      "joseph  |  senka poetess udinese albom machina\n",
      "mathematics  |  tigress kinetic curators supersessionism parodying\n",
      "recorded  |  langston catapult leaky triumphed subroutine\n",
      "defense  |  ericaceae flagstaff posek francs carve\n",
      "bbc  |  constitutes complexion naturae repudiating allo\n",
      "engine  |  panza pacific restful palatine snowmobiles\n",
      "governor  |  code hazing siam seem renner\n",
      "report  |  nilgiri outcry length shuts gravesite\n",
      "...\n",
      "will  |  invoices identifications kochi lachmann aalborg\n",
      "system  |  manat buoyant osmond enhanced boer\n",
      "has  |  priscillian torque ansi madras winged\n",
      "six  |  milk marten lumen palomar congratulate\n",
      "or  |  longa parsley descendents mingw goddamn\n",
      "may  |  cafiero turf fluorine stole closeness\n",
      "s  |  piece grimmett chocolate tricking blamed\n",
      "these  |  hair oscillating hashem epimetheus chronicled\n",
      "test  |  rink fontvieille runways oakland exchequer\n",
      "animals  |  poets trieste reversing ito michaux\n",
      "woman  |  wore skilful autumn kills worldwide\n",
      "older  |  carburetor guarani incessantly lucidity wilhelm\n",
      "rise  |  practices staining similarity hipc spit\n",
      "freedom  |  expounding downers juries phytoplankton albert\n",
      "applications  |  kyd snowdon starkey wingtip pba\n",
      "heavy  |  possibility repubblica pivot dale darya\n",
      "...\n",
      "been  |  repudiating electoral charlize cloudy piercings\n",
      "have  |  theirs disseminating strikeouts familial reductionist\n",
      "the  |  conspiracies ousmane eno rdx lifespans\n",
      "had  |  har kenji thebaid partitioning gene\n",
      "known  |  accrue sweetcorn methanol rested sadness\n",
      "is  |  unrealized anathema rodham trope desolation\n",
      "history  |  physique expropriated alcindor eight timber\n",
      "was  |  indictments kermit griffey gama samara\n",
      "quite  |  goldsmiths junnin balke chandrasekhar abusing\n",
      "road  |  conspiracies inherits peeling flanker fuerzas\n",
      "universe  |  titration trackless finlandization misfits dks\n",
      "running  |  misplaced dragging copps senado ethnologue\n",
      "animals  |  poets trieste ito reversing michaux\n",
      "construction  |  deny sociologists tether cauchon agnostic\n",
      "governor  |  code hazing siam seem enfant\n",
      "writers  |  reorganised lemonade prolifically hope multiplications\n",
      "...\n",
      "but  |  sisters jobs duelling ziyad paralipomena\n",
      "have  |  theirs disseminating strikeouts familial reductionist\n",
      "years  |  genuine jealousy gip comit anheuser\n",
      "new  |  tibetans leyden bgcolor decomposing maius\n",
      "who  |  namely furiously dreyer markt coalition\n",
      "such  |  samkhya curonian icy unleashes toussaint\n",
      "however  |  audiogalaxy sausage timpani gawk suva\n",
      "would  |  sov sinh gainsbourg ohka metered\n",
      "shown  |  abu monotheist monogamy releasing shaved\n",
      "institute  |  lombardic aquilonia taxpayer arabic rushed\n",
      "police  |  intimate leninism started exercising repressor\n",
      "http  |  sarkozy ord murakami hunmin deepened\n",
      "construction  |  deny agnostic cauchon sociologists expectancies\n",
      "proposed  |  decade jour grimoire explains pragmatically\n",
      "hold  |  pregnant ryaku numbering sainsbury lemnian\n",
      "instance  |  aktion zedillo populated consulted laeken\n",
      "...\n",
      "will  |  invoices identifications kochi aalborg lachmann\n",
      "between  |  warmblood angus liberals emergency attributed\n",
      "at  |  vegetative eagerness approve oup culturally\n",
      "have  |  theirs disseminating strikeouts familial reductionist\n",
      "three  |  nonempty kona warhol authorizes nonprofit\n",
      "six  |  milk marten congratulate lumen palomar\n",
      "about  |  automated corrino solicits granitic centralization\n",
      "s  |  piece grimmett chocolate tricking kaliningrad\n",
      "experience  |  igreja undesired raid carolinas absolute\n",
      "except  |  maasai jacobus reprint ritchie colette\n",
      "file  |  adad sheldrake chagos metastability deathbed\n",
      "defense  |  ericaceae flagstaff posek francs eius\n",
      "smith  |  hydrogenation fetches plumb bologna abrogated\n",
      "notes  |  referrals jericho geometer configure beavis\n",
      "existence  |  shangri shortcut militaristic spokesmen necklaces\n",
      "prince  |  occidentalis macneille punctuated langland gzip\n",
      "...\n",
      "be  |  afram auburn unorthodox skyrocketed after\n",
      "th  |  tackled express gasket platinum chalk\n",
      "more  |  nihilism sld lui gray orelli\n",
      "four  |  oilseed moderator digweed enhancement sorrow\n",
      "system  |  manat buoyant enhanced osmond boer\n",
      "used  |  kaiserreich lacking sibylla tenn cimbri\n",
      "is  |  unrealized trope rodham anathema desolation\n",
      "that  |  insular medica vv agnesi rest\n",
      "brother  |  rebuked plus rey repopulated nafta\n",
      "shown  |  abu monotheist monogamy releasing shaved\n",
      "animals  |  poets trieste ito reversing michaux\n",
      "professional  |  nz engrailed muhi caco fiasco\n",
      "grand  |  sensationalist ille academic helles orson\n",
      "san  |  pendleton josephus beyer damian reappear\n",
      "older  |  carburetor guarani incessantly lucidity wilhelm\n",
      "hit  |  balkan rahxephon confessing countdown operatorname\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for  |  fire shamir toscana cortege bums\n",
      "has  |  priscillian ansi madras lep torque\n",
      "two  |  wrested hailie dail confusion gpl\n",
      "seven  |  labelled gerrit rachmaninoff pullback dialogs\n",
      "american  |  fenech tsvetaeva wacky shastri romania\n",
      "called  |  larp enthroned unconnected lenition cellar\n",
      "between  |  warmblood angus liberals emergency attributed\n",
      "there  |  raffarin baldness den macfarquhar newt\n",
      "question  |  saurischia jarrett lauded picchi other\n",
      "operations  |  nestled outings tuileries snags tsuburaya\n",
      "grand  |  sensationalist ille academic helles gateless\n",
      "operating  |  delete moser dodgers haller equating\n",
      "woman  |  wore skilful autumn kills worldwide\n",
      "liberal  |  needle powerpuff eyeball dividends incited\n",
      "channel  |  terrorists dares idealist eels stanislavsky\n",
      "pope  |  coelacanth gec science milman mines\n",
      "...\n",
      "war  |  inspiring crumbs crutches kamchatsky expecting\n",
      "so  |  optimised mushes southwark anta reus\n",
      "after  |  lamanites conforms entire be fount\n",
      "people  |  pharmacology suomenlinna intentions lemmy insurgents\n",
      "s  |  piece grimmett tricking chocolate blamed\n",
      "zero  |  kit nucleoside ede gender impeachments\n",
      "up  |  support krishnan powerhouse envelop cogency\n",
      "will  |  invoices identifications kochi aalborg lassa\n",
      "smith  |  hydrogenation plumb fetches bologna abrogated\n",
      "file  |  adad sheldrake chagos metastability deathbed\n",
      "shown  |  abu monogamy monotheist releasing shaved\n",
      "writers  |  reorganised lemonade prolifically hope multiplications\n",
      "ice  |  winsor cotabato communicating navigator nondeterministic\n",
      "institute  |  lombardic aquilonia taxpayer arabic rushed\n",
      "consists  |  unpredictable correspondance sympathetic parenthesis nh\n",
      "award  |  recommendation musicological snack plumb esas\n",
      "...\n",
      "its  |  stanford lorien stars defile vilayet\n",
      "most  |  domestic congratulated interested iqbal utilize\n",
      "state  |  chernenko hayastan aquinas uat vulpes\n",
      "he  |  disproved fry iarc screwed remick\n",
      "i  |  ensure buckwheat grands jakobson speakers\n",
      "th  |  tackled express gasket platinum chalk\n",
      "two  |  wrested instabilities gpl hailie heaney\n",
      "is  |  unrealized trope rodham vasquez desolation\n",
      "magazine  |  stanley pedals quasimodo sdf neal\n",
      "pre  |  catacombs murad hegelian shiraz rapp\n",
      "additional  |  endomorphisms kober cong analecta belgrade\n",
      "mathematics  |  kinetic tigress supersessionism curators parodying\n",
      "operating  |  delete moser haller magnus equating\n",
      "something  |  salvage foundationalism albano abrogate exams\n",
      "recorded  |  langston catapult leaky triumphed subroutine\n",
      "road  |  conspiracies inherits peeling flanker stressful\n",
      "...\n",
      "b  |  immortalized heteronormativity newly archipelagoes pedagogue\n",
      "most  |  domestic interested congratulated iqbal utilize\n",
      "that  |  insular medica translation vv agnesi\n",
      "its  |  stanford lorien stars defile vilayet\n",
      "first  |  recuperate resolved bandwidth high desist\n",
      "see  |  guide centred medal merits basis\n",
      "two  |  wrested instabilities gpl hailie instill\n",
      "for  |  fire shamir cortege lutwidge bums\n",
      "construction  |  deny agnostic cauchon sociologists tether\n",
      "engine  |  pacific panza restful palatine snowmobiles\n",
      "orthodox  |  tycoon rendering hawar paramount rahula\n",
      "instance  |  aktion zedillo populated consulted laeken\n",
      "road  |  conspiracies inherits peeling flanker stressful\n",
      "except  |  maasai mikhalkov reprint jacobus colette\n",
      "numerous  |  dassault frown famine sewage eastbourne\n",
      "taking  |  nolan companion documented unrealized dehydrogenase\n",
      "...\n",
      "can  |  apprenticeship travel linkage soissons mouths\n",
      "over  |  exiled christoph accentuated acclamation attacked\n",
      "s  |  piece grimmett tricking blamed sines\n",
      "had  |  har kenji thebaid partitioning blames\n",
      "during  |  localization ewen persians reinventing beagle\n",
      "state  |  chernenko hayastan aquinas uat afips\n",
      "no  |  utilization fundamental aime stumbles longford\n",
      "five  |  mcginley sweeping ketchum to gopal\n",
      "institute  |  lombardic aquilonia taxpayer arabic rushed\n",
      "behind  |  novel xix finished ql master\n",
      "construction  |  deny agnostic cauchon sociologists isotropy\n",
      "active  |  horner earthsea startposition drapery michell\n",
      "articles  |  zulus deformed filming hunters rewrote\n",
      "scale  |  symbolic speculators ribao earley telekom\n",
      "defense  |  ericaceae posek flagstaff francs eius\n",
      "bbc  |  constitutes complexion naturae synchronization cadaver\n",
      "...\n",
      "all  |  ten activities antisymmetry ofdm ust\n",
      "world  |  marquise responsive chamorro tour tungusic\n",
      "over  |  exiled christoph accentuated attacked hatta\n",
      "often  |  munition marqu producers apex dipping\n",
      "new  |  tibetans shaku maius decomposing pelicans\n",
      "seven  |  the gargoyle rachmaninoff of taught\n",
      "people  |  pharmacology suomenlinna lemmy intentions mozambican\n",
      "in  |  the of pledged royalty nagasaki\n",
      "consists  |  unpredictable correspondance sympathetic parenthesis nh\n",
      "mainly  |  abidjan combats kedah dnestr apaches\n",
      "stage  |  interpol ignazio tte espa turbocharged\n",
      "shown  |  abu monogamy monotheist releasing shaved\n",
      "assembly  |  cocktail innards hoard nymphs garz\n",
      "grand  |  sensationalist ille academic helles gateless\n",
      "hit  |  balkan rahxephon confessing risen countdown\n",
      "marriage  |  against paracelsus samarium ultimate discordant\n",
      "...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6221a514b3e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cuda-env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cuda-env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(train_on_gpu)\n",
    "\n",
    "epochs = 5\n",
    "embedding_dim = 300\n",
    "batch_size = 512\n",
    "window_size = 5\n",
    "step = 0\n",
    "print_every = 500\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "model = SkipGram(len(set(int_words)), embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.003)\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for e in range(1,epochs+1):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for x,y in get_batches(int_words, batch_size, window_size=window_size):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, targets = torch.LongTensor(x), torch.LongTensor(y)\n",
    "        \n",
    "        #inputs_same_as_target = (inputs == targets)\n",
    "        #if torch.sum(inputs_same_as_target)>0:\n",
    "        #    print(inputs, targets)\n",
    "        #print(\"stop1\")\n",
    "        if train_on_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = inputs.cuda()\n",
    "            \n",
    "        out = model(inputs)\n",
    "        \n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #_, topk = out.topk(1, dim=1)\n",
    "        #input_same_as_pred = (inputs == topk)\n",
    "        #if torch.sum(input_same_as_pred)>0:\n",
    "        #    print(inputs, topk)\n",
    "        #print(\"stop2\")\n",
    "        running_loss += loss.item()\n",
    "        step+=1\n",
    "        \n",
    "        if step%print_every==0:\n",
    "            valid_examples, similarities = cosine_similarity(model.embed, valid_size=16, valid_window=100,\n",
    "                                                             device='cuda' if train_on_gpu else \"cpu\")\n",
    "            _, topk = similarities.topk(6, dim=1)\n",
    "            valid_examples, topk = valid_examples.to('cpu'), topk.to('cpu')\n",
    "            \n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in topk[ii,1:]]\n",
    "                print(int_to_vocab[valid_idx.item()],' | ', *closest_words)\n",
    "            print('...')\n",
    "        \n",
    "    print(f\"Epoch {e}, loss: {running_loss/len(int_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the word vectors\n",
    "\n",
    "Below we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project these vectors into two dimensions while preserving local stucture. Check out [this post from Christopher Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) to learn more about T-SNE and other ways to visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.embed.weight.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viz_words = 600\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
